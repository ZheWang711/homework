# Homework 2
Zhe Wang
A53097553

### Question 1

#### Naive bayes solution
The following code describes the procedure of predicting a given document using naive Bayes solution.

* `X` is the given document in a hashmap format. For any item in `X`, its key is the label of a word, and the value is the number of times of that word appears in that document.
* `A` is the probability matrix trained by training data. `A[i][j]` is the *log* probability of word `j + 1` appears in class `i + 1`
* `Pi` is the fraction matrix. `Pi[i]` is the *log* fraction of document `i+1` in training data ($$log\pi_i$$).
* The followed code computes and return $$argmax_j(log(\pi_j) + A'X)$$ where $$A'$$ is the matrix formed by extracting columns corresponding with $$X$$'s word label. 

```Python
def predict(X, A, Pi, np):
    X = [i for i in X.items()]
    X.sort()
    index = [x[0] for x in X]
    value = [x[1] for x in X]
    tmp = Pi + np.dot(A[:,index], value)
    return np.argmax(tmp) + 1
```
The error rate of above method (no optimization) is 21.89%.

#### Several Optimaization results


| Method | error rate |
| -- | -- |
| remove most frequent words in English | 20.23% |
| replace frequency f as log(1+f) | 20.97% |
| combination of the two above | 20.11% |
| The third one with removing most frequent words in training set  | 18.92%, <br> frequency bar = 0.0007 |

The optimal frequent bar is acquired by the following **frequent bar -- error rate** figure.

![](output_29_1.png)

Therefore, the final model is composed with 3 optimization methods (removing frequent words in English, removing frequent words in training set, and replacing f as log(1+f)) , and achieves error rate to be 18.92%.



---


### Question2

In the case of binary classification with an abstain option, we only have 3 possible choice for $$y$$ given $$x$$.

* Case 1: Predicting $$y = 1$$.  
Then $$E(cost)= 0*P(y=1|x) +1*P(y=0|x) = 1- \eta(x)$$

* Case 2: Predicting $$y=0$$.  
Then $$E(cost)=1*P(y=1|x) +0*P(y=0|x) = \eta(x)$$

* Case 3: Predicting $$y=abstain$$, the expected cost is $$\theta$$.

Therefore, in order to get minimum expected cost, we can choose $$y$$ in the way such that the corresponding cost smallest among the above 3 cases.

$$h^*(x) = \begin{cases}
0 \quad \text{if} \quad \eta(x) < 1- \eta(x) \text{ and } \eta(x) < \theta \\
1 \quad \text{if} \quad 1-\eta(x) < \eta(x) \text{ and } 1 - \eta(x) < \theta \\
abstain \quad \text{otherwise} 
\end{cases}
$$